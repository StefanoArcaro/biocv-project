{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 1234\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tensorflow is using GPU\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_samples_loaded, T2_samples_loaded, T1_labels_loaded, T2_labels_loaded = utils.load_preprocessed_data_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_data_samples, T2_data_samples, T1_data_labels, T2_data_labels = utils.prepare_data_for_training(T1_samples_loaded, T2_samples_loaded, T1_labels_loaded, T2_labels_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertically stack the T1 and T2 data (samples and labels)\n",
    "data_samples = np.vstack((T1_data_samples, T2_data_samples))\n",
    "data_labels = np.vstack((T1_data_labels, T2_data_labels))\n",
    "\n",
    "# Print the shape of the data\n",
    "print(data_samples.shape)\n",
    "print(data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = utils.split_data_for_training(data_samples, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train[0].shape\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model and a custom IOU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the UNet model\n",
    "unet_model = utils.get_unet_model(input_shape=T1_data_samples[0].shape, num_classes=len(np.unique(T1_data_labels)))\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom metric class for mean intersection over union (IoU)\n",
    "class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes=None, name=\"mean_iou\", dtype=None):\n",
    "        super(UpdatedMeanIoU, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up RAM\n",
    "del T1_samples_loaded, T2_samples_loaded, T1_labels_loaded, T2_labels_loaded, T1_data_samples, T2_data_samples, T1_data_labels, T2_data_labels, data_samples, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "input_shape = X_train[0].shape\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "\n",
    "# Define compile parameters\n",
    "loss = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "metrics = ['accuracy', UpdatedMeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# Define callbacks\n",
    "patience = 30\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_mean_iou', mode='max', patience=patience, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_mean_iou', mode='max', factor=0.1, patience=patience-5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "unet_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = unet_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_pipeline = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the augmentation pipeline\n",
    "sample = X_train[10]\n",
    "label = y_train[10]\n",
    "augmented = augmentation_pipeline(image=sample, mask=label)\n",
    "augmented_image = augmented['image']\n",
    "augmented_label = augmented['mask']\n",
    "\n",
    "# Plot the original image and the augmented image, together with their labels\n",
    "utils.plot_sample(sample, label, plot_separately=True)\n",
    "utils.plot_sample(augmented_image, augmented_label, plot_separately=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to apply the augmentation pipeline to a dataset\n",
    "def augment_function(image, label):\n",
    "    augmented = augmentation_pipeline(image=sample, mask=label)\n",
    "    augmented_image = augmented['image']\n",
    "    augmented_label = augmented['mask']\n",
    "    augmented_image = tf.cast(augmented_image, tf.float32)\n",
    "    augmented_label = tf.cast(augmented_label, tf.int32)\n",
    "    return augmented_image, augmented_label\n",
    "\n",
    "# Create a function to apply the augmentation pipeline to a dataset\n",
    "def process_data(image, label):\n",
    "    augmented_image, augmented_label = tf.numpy_function(func=augment_function, inp=[image, label], Tout=[tf.float32, tf.int32])\n",
    "    return augmented_image, augmented_label\n",
    "\n",
    "# Create the augmented dataset\n",
    "augmented_dataset = X_train\n",
    "augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biocv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
